{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_rnn_test1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA6ywQGcf7MR"
      },
      "source": [
        "This text classification tutorial trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis.\n",
        "\n",
        "Source: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw8dg3Ptes-g",
        "outputId": "95b433c6-0cfa-49d3-f3d4-b7e8695092a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install -q tensorflow_datasets\n",
        "!pip3 install mlflow"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/2d/7fa1f6e310ded489d943ea20cd7977a9867cb8d81b526d9c9460ce4a5b39/mlflow-1.11.0-py3-none-any.whl (13.9MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9MB 303kB/s \n",
            "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/d9/01d0bda8d764be9e7448a41c34eec002e67e88e0d558b80d0ba922b4f7f0/databricks-cli-0.13.0.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hCollecting gorilla\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/56/5a683944cbfc77e429c6f03c636ca50504a785f60ffae91ddd7f5f7bb520/gorilla-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
            "Collecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.2)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/d5/8a046d683c2cc084b6a502812827ede69b1064f95d93f94b83f809b21723/prometheus_flask_exporter-0.18.1.tar.gz\n",
            "Collecting gunicorn; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
            "Collecting sqlalchemy<=1.3.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/47/35edeb0f86c0b44934c05d961c893e223ef27e79e1f53b5e6f14820ff553/SQLAlchemy-1.3.13.tar.gz (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.23.0)\n",
            "Collecting azure-storage-blob>=12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/84/7e51b3e1156bcb89a20b9ec641d4fced4800aa79daac3a403898c32046be/azure_storage_blob-12.5.0-py2.py3-none-any.whl (326kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.18.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.12.4)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.4.1)\n",
            "Collecting alembic<=1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.8MB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/8c/8d42dbd83679483db207535f4fb02dc84325fa78b290f057694b057fcd21/docker-4.3.1-py2.py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.15.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.11.2)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.6/dist-packages (from prometheus-flask-exporter->mlflow) (0.8.0)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.6/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (50.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2020.6.20)\n",
            "Collecting msrest>=0.6.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/f5/9e315fe8cb985b0ce052b34bcb767883dc739f46fadb62f05a7e6d6eedbe/msrest-0.6.19-py2.py3-none-any.whl (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.5MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a2/6565c5271a79e3c96d7a079053b4d8408a740d4bf365f0f5f244a807bd09/cryptography-3.2.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 35.4MB/s \n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/fa/46974f4a7ad78b27e3eda8a573cc0c2508849f0d7d360b61c07cc5b46014/azure_core-1.8.2-py2.py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.8MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.5MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob>=12.0->mlflow) (1.3.0)\n",
            "Collecting isodate>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0->mlflow) (1.14.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-storage-blob>=12.0->mlflow) (3.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob>=12.0->mlflow) (2.20)\n",
            "Building wheels for collected packages: databricks-cli, prometheus-flask-exporter, sqlalchemy, alembic\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.13.0-cp36-none-any.whl size=100347 sha256=64585b38113e1ab80b0dffc5030481a5bb88425994ee2d8f37e08cda2f47401e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/ba/45/11e7abe40b8e21f4aab12a3051e269b24d17f2b288bb7418d5\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-cp36-none-any.whl size=17157 sha256=5da92f2f0bcfaca950ba057e693275e6cea1885bd02dc3c234aba0344195c6a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/1f/b8/66bd9bc3a9d6c6987ff6c4dfeb6f1fe97b5a0e5ed5849c0437\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.3.13-cp36-cp36m-linux_x86_64.whl size=1217162 sha256=5128f7271a6b1e98bfbf58001d9ae11a92dd53adae2725251f1c23bbe06a28de\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/35/98/4c9cb3fd63d21d5606b972dd70643769745adf60e622467b71\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=d8acd35bd305f29883004c80529e316b3cf02bd73fa8b8e9119deea43b641d8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
            "Successfully built databricks-cli prometheus-flask-exporter sqlalchemy alembic\n",
            "Installing collected packages: databricks-cli, gorilla, querystring-parser, smmap, gitdb, gitpython, prometheus-flask-exporter, gunicorn, sqlalchemy, isodate, msrest, cryptography, azure-core, azure-storage-blob, Mako, python-editor, alembic, websocket-client, docker, mlflow\n",
            "  Found existing installation: SQLAlchemy 1.3.20\n",
            "    Uninstalling SQLAlchemy-1.3.20:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.20\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.1 azure-core-1.8.2 azure-storage-blob-12.5.0 cryptography-3.2.1 databricks-cli-0.13.0 docker-4.3.1 gitdb-4.0.5 gitpython-3.1.11 gorilla-0.3.0 gunicorn-20.0.4 isodate-0.6.0 mlflow-1.11.0 msrest-0.6.19 prometheus-flask-exporter-0.18.1 python-editor-1.0.4 querystring-parser-1.2.4 smmap-3.0.4 sqlalchemy-1.3.13 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFpV979FgEC7"
      },
      "source": [
        "import numpy as np\n",
        "import mlflow\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FzYVxz3gJl8"
      },
      "source": [
        "#config\n",
        "\n",
        "NGROK_URL='http://ec2-3-133-154-208.us-east-2.compute.amazonaws.com:5001/'\n",
        "EXPERIMENT='nlp/rnn/IMDB_movie_sentiment'\n",
        "EPOCHS=10\n",
        "VERSION=1\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE=1000\n",
        "\n",
        "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer=tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "SAVE_PATH='saved_model'\n",
        "%mkdir -p saved_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbG0pn4zkqGl"
      },
      "source": [
        "#Mlflow settings\n",
        "\n",
        "#set MLflow server \n",
        "mlflow.set_tracking_uri(NGROK_URL)\n",
        "#Set experiment\n",
        "if mlflow.get_experiment_by_name(EXPERIMENT) != None:\n",
        "    exp_id = mlflow.set_experiment(EXPERIMENT)\n",
        "else: \n",
        "    exp_id = mlflow.create_experiment(EXPERIMENT)\n",
        "\n",
        "#Close active runs\n",
        "if mlflow.active_run():\n",
        "    mlflow.end_run()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeV5Zf8OgnHd"
      },
      "source": [
        "tags={}\n",
        "tags['TYPE']='NLP'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKrSfLygq1N"
      },
      "source": [
        "#Import matplotlib and create a helper function to plot graphs:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbSIhh2ngtzw",
        "outputId": "e9cbe902-dffb-44df-fa7d-97f513b08abb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Download the dataset using TFDS. \n",
        "\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteAI4361/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteAI4361/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteAI4361/imdb_reviews-unsupervised.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLIPuHIagxTI",
        "outputId": "364f96e8-3aaa-41dc-deb8-f9ab0eda55a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Initially this returns a dataset of (text, label pairs):\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtBujTOEgzbo"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yEjNzYfhNub",
        "outputId": "b77999b3-5b3e-4545-f3bb-37aa1d5f380d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts:  [b\"This inept adaptation of arguably one of Martin Amis's weaker novels fails to even draw comparisons with other druggy oeuvres such as Requiem For A Dream or anything penned by Irvine Walsh as it struggles to decide whether it is a slap-stick cartoon or a hyper-realistic hallucination.<br /><br />Boringly directed by William Marsh in over-saturated hues, a group of public school drop-outs converge in a mansion awaiting the appearance of three American friends for a weekend of decadent drug-taking. And that's it. Except for the ludicrous sub-plot soon-to-be-the-main-plot nonsense about an extremist cult group who express themselves with the violent killings of the world's elite figures, be it political or pampered. Within the first reel you know exactly where this is going.<br /><br />What is a talented actor like Paul Bettany doing in this tiresome, badly written bore? Made prior to his rise to fame and Jennifer Connelly one can be assured that had he been offered this garbage now he'd have immediately changed agents! Avoid.\"\n",
            " b'9/10- 30 minutes of pure holiday terror. Okay, so it\\'s not that scary. But it sure is fun.<br /><br />The Crypt Keeper (John Kassir) tales a tale of holiday FEAR, giving us all Christmas Goose... GosseBUMPS That is. Bwahahahahha. You should really be careful what you AXE Santa for. Have a Scary Christmas and a Happy New Fear. Okay I\\'ll stop.<br /><br />Okay, so in the story, a greedy wife (Best screamer in the world, Mary Ellen Trainor) kills her husband (Marshall Bell, the coach who gets towel whipped to death in ANOES 2) for the money. BUT, her plan is ruined when a crazy killer dressed in a Santa suit (Larry \"Dr. Giggles\" Drake) comes her way.<br /><br />If you look it up on YouTube, you can watch it for free, but most of you have already seen this (my third viewing). But if you haven\\'t seen it, I suggest you do.'\n",
            " b'The title overstates the content of this movie somewhat, which might lead to some unrealized expectations. Frankly speaking, there\\'s very little \"panic in the streets\" to be seen here. In fact, throughout the movie very few people actually know that there\\'s a murderer on the loose who may well be spreading the plague to everyone and anyone he encounters. Having said that, what we do have here is a very well done story with a level of suspense that starts out reasonably high anyway (because, unlike the people \"in the streets\", the viewer knows what\\'s going on) and that director Elia Kazan builds very deliberately. As the plague-infected killer is sought, one of the more interesting sidebars I found was the developing relationship between Dr. Reed (Richard Widmark) and Police Captain Warren (Paul Douglas). At the beginning, the two really don\\'t like each other, even though they have to work together. By the end, they\\'ve forged a real bond of respect for each other. Kazan did a good job with that.<br /><br />Pretty much all the performances here were excellent. Widmark and Douglas were great, and I was quite taken with a very early look at Jack Palance playing what would become his typical \"heavy\" role. I found very little to criticize here. Perhaps Barbara Bel Geddes came across as a little bit flat as Reed\\'s wife Nancy, but her role wasn\\'t really central to the story. All in all, an excellent piece of work. 9/10']\n",
            "\n",
            "labels:  [0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJR9tlQhhTm7"
      },
      "source": [
        "The raw text loaded by tfds needs to be processed before it can be used in a model. The simplest way to process text for training is using the experimental.preprocessing.TextVectorization layer. \n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method.\n",
        "\n",
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency.\n",
        "\n",
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqi2NSSiIRf",
        "outputId": "966cc167-649d-4fa0-effa-d25beab9b313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWflDrydiQWU",
        "outputId": "6eae67e4-5a38-4064-e16a-134d9b324544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 11,   1,   1, ...,   0,   0,   0],\n",
              "       [  1,   1, 233, ...,   0,   0,   0],\n",
              "       [  2, 422,   1, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xwmCfrliSim",
        "outputId": "380bf9a7-d092-487b-b2e7-3451ea7fe069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  b\"This inept adaptation of arguably one of Martin Amis's weaker novels fails to even draw comparisons with other druggy oeuvres such as Requiem For A Dream or anything penned by Irvine Walsh as it struggles to decide whether it is a slap-stick cartoon or a hyper-realistic hallucination.<br /><br />Boringly directed by William Marsh in over-saturated hues, a group of public school drop-outs converge in a mansion awaiting the appearance of three American friends for a weekend of decadent drug-taking. And that's it. Except for the ludicrous sub-plot soon-to-be-the-main-plot nonsense about an extremist cult group who express themselves with the violent killings of the world's elite figures, be it political or pampered. Within the first reel you know exactly where this is going.<br /><br />What is a talented actor like Paul Bettany doing in this tiresome, badly written bore? Made prior to his rise to fame and Jennifer Connelly one can be assured that had he been offered this garbage now he'd have immediately changed agents! Avoid.\"\n",
            "Round-trip:  this [UNK] [UNK] of [UNK] one of [UNK] [UNK] [UNK] [UNK] fails to even [UNK] [UNK] with other [UNK] [UNK] such as [UNK] for a dream or anything [UNK] by [UNK] [UNK] as it [UNK] to [UNK] whether it is a [UNK] [UNK] or a [UNK] [UNK] br [UNK] directed by [UNK] [UNK] in [UNK] [UNK] a group of [UNK] school [UNK] [UNK] in a [UNK] [UNK] the [UNK] of three american friends for a [UNK] of [UNK] [UNK] and thats it except for the [UNK] [UNK] [UNK] [UNK] about an [UNK] [UNK] group who [UNK] themselves with the [UNK] [UNK] of the [UNK] [UNK] [UNK] be it political or [UNK] within the first [UNK] you know exactly where this is [UNK] br what is a [UNK] actor like paul [UNK] doing in this [UNK] badly written [UNK] made [UNK] to his [UNK] to [UNK] and [UNK] [UNK] one can be [UNK] that had he been [UNK] this [UNK] now [UNK] have [UNK] [UNK] [UNK] avoid                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            "\n",
            "Original:  b'9/10- 30 minutes of pure holiday terror. Okay, so it\\'s not that scary. But it sure is fun.<br /><br />The Crypt Keeper (John Kassir) tales a tale of holiday FEAR, giving us all Christmas Goose... GosseBUMPS That is. Bwahahahahha. You should really be careful what you AXE Santa for. Have a Scary Christmas and a Happy New Fear. Okay I\\'ll stop.<br /><br />Okay, so in the story, a greedy wife (Best screamer in the world, Mary Ellen Trainor) kills her husband (Marshall Bell, the coach who gets towel whipped to death in ANOES 2) for the money. BUT, her plan is ruined when a crazy killer dressed in a Santa suit (Larry \"Dr. Giggles\" Drake) comes her way.<br /><br />If you look it up on YouTube, you can watch it for free, but most of you have already seen this (my third viewing). But if you haven\\'t seen it, I suggest you do.'\n",
            "Round-trip:  [UNK] [UNK] minutes of [UNK] [UNK] [UNK] okay so its not that scary but it sure is [UNK] br the [UNK] [UNK] john [UNK] [UNK] a tale of [UNK] [UNK] giving us all christmas [UNK] [UNK] that is [UNK] you should really be [UNK] what you [UNK] [UNK] for have a scary christmas and a happy new [UNK] okay ill [UNK] br okay so in the story a [UNK] wife best [UNK] in the world [UNK] [UNK] [UNK] [UNK] her husband [UNK] [UNK] the [UNK] who gets [UNK] [UNK] to death in [UNK] 2 for the money but her [UNK] is [UNK] when a crazy killer [UNK] in a [UNK] [UNK] [UNK] dr [UNK] [UNK] comes her [UNK] br if you look it up on [UNK] you can watch it for free but most of you have already seen this my third viewing but if you havent seen it i [UNK] you do                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "\n",
            "Original:  b'The title overstates the content of this movie somewhat, which might lead to some unrealized expectations. Frankly speaking, there\\'s very little \"panic in the streets\" to be seen here. In fact, throughout the movie very few people actually know that there\\'s a murderer on the loose who may well be spreading the plague to everyone and anyone he encounters. Having said that, what we do have here is a very well done story with a level of suspense that starts out reasonably high anyway (because, unlike the people \"in the streets\", the viewer knows what\\'s going on) and that director Elia Kazan builds very deliberately. As the plague-infected killer is sought, one of the more interesting sidebars I found was the developing relationship between Dr. Reed (Richard Widmark) and Police Captain Warren (Paul Douglas). At the beginning, the two really don\\'t like each other, even though they have to work together. By the end, they\\'ve forged a real bond of respect for each other. Kazan did a good job with that.<br /><br />Pretty much all the performances here were excellent. Widmark and Douglas were great, and I was quite taken with a very early look at Jack Palance playing what would become his typical \"heavy\" role. I found very little to criticize here. Perhaps Barbara Bel Geddes came across as a little bit flat as Reed\\'s wife Nancy, but her role wasn\\'t really central to the story. All in all, an excellent piece of work. 9/10'\n",
            "Round-trip:  the title [UNK] the [UNK] of this movie somewhat which might lead to some [UNK] [UNK] [UNK] [UNK] theres very little [UNK] in the [UNK] to be seen here in fact throughout the movie very few people actually know that theres a [UNK] on the [UNK] who may well be [UNK] the [UNK] to everyone and anyone he [UNK] having said that what we do have here is a very well done story with a level of suspense that starts out [UNK] high anyway because unlike the people in the [UNK] the viewer knows whats going on and that director [UNK] [UNK] [UNK] very [UNK] as the [UNK] killer is [UNK] one of the more interesting [UNK] i found was the [UNK] relationship between dr [UNK] richard [UNK] and police [UNK] [UNK] paul [UNK] at the beginning the two really dont like each other even though they have to work together by the end [UNK] [UNK] a real [UNK] of [UNK] for each other [UNK] did a good job with [UNK] br pretty much all the performances here were excellent [UNK] and [UNK] were great and i was quite taken with a very early look at jack [UNK] playing what would become his typical [UNK] role i found very little to [UNK] here perhaps [UNK] [UNK] [UNK] came across as a little bit [UNK] as [UNK] wife [UNK] but her role wasnt really [UNK] to the story all in all an excellent piece of work [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAPkfqW6ioCj"
      },
      "source": [
        "Above is a diagram of the model.\n",
        "\n",
        "1. This model can be build as a tf.keras.Sequential.\n",
        "\n",
        "2. The first latyer is the encoder, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a tf.keras.layers.Dense layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "The tf.keras.layers.Bidirectional wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output.\n",
        "\n",
        "* The main advantage to a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.\n",
        "\n",
        "* The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two layers.Dense do some final processing, and convert from this vector representation to a single logit as the classification output.\n",
        "\n",
        "The code to implement this is below.\n",
        "\n",
        "\n",
        "<center><img src=\"https://www.tensorflow.org/tutorials/text/images/bidirectional.png\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVaAIjAVilcQ"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JptZYuGZjzHk"
      },
      "source": [
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK6H10Kpj1fP",
        "outputId": "9e1ef179-dacb-4819-d9bc-4d8ca3f26554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with mlflow.start_run(run_id=None, experiment_id=exp_id, run_name=None, nested=False): \n",
        "  history = model.fit(train_dataset, epochs=EPOCHS,\n",
        "                    validation_data=test_dataset, \n",
        "                    validation_steps=30)\n",
        "  #mlflow autolog\n",
        "      #mlflow.tensorflow.autolog()\n",
        "\n",
        "  #Set tags\n",
        "  mlflow.set_tags(tags)\n",
        "\n",
        "  #mlflow logging\n",
        "  \n",
        "  # log parameters\n",
        "  mlflow.log_param(\"epochs\", EPOCHS)\n",
        "  mlflow.log_param(\"loss_function\", str(loss))\n",
        "\n",
        "  # log metrics\n",
        "  for epoch in range(0, EPOCHS):\n",
        "    mlflow.log_metric(\"accuracy\", hist['accuracy'][epoch])\n",
        "    mlflow.log_metric(\"loss\",  hist['loss'][epoch])\n",
        "    mlflow.log_metric(\"val_accuracy\",  hist['val_accuracy'][epoch])\n",
        "    mlflow.log_metric(\"val_loss\",  hist['val_loss'][epoch])\n",
        "  \n",
        "  #results=evaluate_model() #TODO\n",
        "  #mlflow.log_metric(\"average_loss\", results[0])\n",
        "  #mlflow.log_metric(\"average_acc\", results[1])\n",
        "  \n",
        "  #log model\n",
        "  #model.save(os.path.join(BASE_DIR, \"models\", \"{}.h5\".format(int(t)))) #HDF5 format\n",
        "  tf.saved_model.save(model, SAVE_PATH) #SavedModel format\n",
        "  #mlflow.tensorflow.log_model(model, 'model') #TODO fix\n",
        "  \n",
        "  # log artifacts (matplotlib images for loss/accuracy)\n",
        "  #mlflow.log_artifacts(model_folder)\n",
        "\n",
        "  mlflow.end_run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "102/391 [======>.......................] - ETA: 8:12 - loss: 0.6925 - accuracy: 0.5021"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ega5mDqekV6n"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM3aKjRXkfa6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}